{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-GRNN\n",
    "\n",
    "Implementation based on paper here:\n",
    "http://aclweb.org/anthology/D15-1167\n",
    "\n",
    "Inspiration for code taken from here:\n",
    "https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/mohsin/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, gc, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Permute, GRU, Conv1D, LSTM, Embedding, Dropout, Activation, CuDNNLSTM, CuDNNGRU, concatenate, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, BatchNormalization, SpatialDropout1D, Dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from functools import reduce\n",
    "from keras.layers import Layer, PReLU, SpatialDropout1D, TimeDistributed\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TweetTokenizer, MWETokenizer, ToktokTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "np.random.seed(786)\n",
    "\n",
    "from SentenceTokenizer import SentenceTokenizer\n",
    "from ZeroMaskedLayer import ZeroMaskedLayer\n",
    "from AttentionLayer import AttentionLayer, AttentionWrapper, AttentionWithContext, Attention\n",
    "from RocAucEvaluation import RocAucEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "utility_path = '../utility/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'{utility_path}crawl-300d-2M.vec'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(series):\n",
    "    return series.apply(lambda s: unicodedata.normalize('NFKC', str(s)))\n",
    "\n",
    "STOP_WORDS = set(stopwords.words( 'english' ))\n",
    "\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    #\"m\": \"am\",\n",
    "    #\"r\": \"are\",\n",
    "    #\"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "}\n",
    "\n",
    "#https://stackoverflow.com/questions/15175142/how-can-i-do-multiple-substitutions-using-regex-in-python\n",
    "def one_xlat(match):\n",
    "        return repl[match.group(0)]\n",
    "    \n",
    "rx = re.compile('|'.join(map(re.escape, repl)))\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(series):\n",
    "    series = unicodeToAscii(series)\n",
    "    series = series.str.lower()\n",
    "    series = series.str.replace(rx, one_xlat)\n",
    "    series = series.str.replace(r\"(\\n){1,}\", \" \")\n",
    "    series = series.str.replace(r\"\\'\", \"\")\n",
    "    series = series.str.replace(r\"\\-\", \"\")\n",
    "    series = series.str.replace(r\"[^0-9a-zA-Z.,!?]+\", \" \")\n",
    "    series = series.str.replace(r\"[.]+\",\".\")\n",
    "    series = series.str.replace(r\"[!]+\",\"!\")\n",
    "    series = series.str.replace(r\"[?]+\",\".\")\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8) (153164, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "\n",
    "#Get validation folds\n",
    "train['target_str'] = reduce(lambda x,y: x+y, [train[col].astype(str) for col in list_classes])\n",
    "train['target_str'] = train['target_str'].replace('110101', '000000').replace('110110','000000')\n",
    "cvlist1 = list(StratifiedKFold(n_splits=10, random_state=786).split(train, train['target_str'].astype('category')))\n",
    "cvlist2 = list(StratifiedShuffleSplit(n_splits=5, test_size=0.05, random_state=786).split(train, train['target_str'].astype('category')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in train, test:\n",
    "    df[\"comment_text\"] = normalizeString(df[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 200000\n",
    "MAX_SENTENCE_LEN = 50\n",
    "MAX_SENTENCES = 10\n",
    "\n",
    "def custome_tokenizer(text):\n",
    "    return [TweetTokenizer().tokenize(sent) for sent in  sent_tokenize(text)]\n",
    "\n",
    "tok = SentenceTokenizer(max_features=MAX_FEATURES, max_sentence_len=MAX_SENTENCE_LEN, max_sentences=MAX_SENTENCES, tokenizer=custome_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = Counter()\n",
    "#s = Counter()\n",
    "#def cnts(x):\n",
    "#    toks = custome_tokenizer(x)\n",
    "#    s.update([len(toks)])\n",
    "#    c.update([len(sent) for sent in toks])\n",
    "#train.comment_text.apply(lambda x: cnts(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 364 ms, total: 2min 12s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = tok.fit_transform(train.comment_text)\n",
    "X_test = tok.transform(test.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 10, 50) (153164, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def initialize_embeddings(filename, tokenizer):\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(filename))\n",
    "\n",
    "    word_index = tokenizer.vocab_idx\n",
    "    nb_words = min(MAX_FEATURES+1, len(word_index)+1)\n",
    "    embedding_matrix = np.zeros((nb_words, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        if i > MAX_FEATURES: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200001, 300)\n",
      "0.0026336309479794836 0.22155647063497153\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = initialize_embeddings(EMBEDDING_FILE, tok)\n",
    "print(embedding_matrix.shape)\n",
    "print(np.mean(embedding_matrix), np.std(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_encoder(x, seq_len, enc_dim, bidirectional=False, rnn_cell='gru'):\n",
    "    \n",
    "    #Select cell to use\n",
    "    if rnn_cell == 'gru':\n",
    "        rnn_enc = GRU(enc_dim, return_sequences=True)\n",
    "    elif rnn_cell == 'lstm':\n",
    "        rnn_enc = LSTM(enc_dim, return_sequences=True)\n",
    "        \n",
    "    #Apply bidirectional is flag in on\n",
    "    if bidirectional:\n",
    "        enc = Bidirectional(rnn_enc)(x)\n",
    "    else:\n",
    "        enc = rnn_enc(x)\n",
    "        \n",
    "def seq_pooling(x, pool_dim=200, pool_type='attention'):\n",
    "    \n",
    "    if pool_type == 'attention':\n",
    "        x = TimeDistributed(Dense(pool_dim))(x)\n",
    "        x = AttentionLayer()(x)\n",
    "        \n",
    "    elif pool_type == 'average':\n",
    "        return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class LSTMGRNN(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, rnn_word_dim=150, rnn_sent_dim=150, dense_dim=256, rnn_type=\"gru\", batch_size=128, epochs=2, bidirectional=False, \n",
    "                 pool_type='all', initial_weights=None, optimizer='adam' ,verbose=1, out_dim=6, callbacks=None,\n",
    "                spatial_drop=0.0, dropout=0.0, mask_zero=True, \n",
    "                gru_kernel_regularization = 0.0001,\n",
    "                gru_recurrent_regularization = 0.0001,\n",
    "                gru_bias_regularization = 0.0001,\n",
    "                embeddings_regularization = 0.0,\n",
    "                ):\n",
    "        \n",
    "        self.rnn_word_dim = rnn_word_dim\n",
    "        self.rnn_sent_dim = rnn_sent_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.rnn_type = rnn_type\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs= epochs\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pool_type = pool_type\n",
    "        self.initial_weights = initial_weights\n",
    "        self.verbose = verbose\n",
    "        self.callbacks = callbacks\n",
    "        self.optimizer = optimizer\n",
    "        self.out_dim = out_dim\n",
    "        self.spatial_drop = spatial_drop\n",
    "        self.dropout = dropout\n",
    "        self.mask_zero = mask_zero\n",
    "        self.gru_kernel_regularization = gru_kernel_regularization\n",
    "        self.gru_recurrent_regularization = gru_recurrent_regularization\n",
    "        self.gru_bias_regularization = gru_bias_regularization\n",
    "        self.embeddings_regularization = embeddings_regularization\n",
    "        \n",
    "    def _build_model(self):\n",
    "        inp = Input(shape=(MAX_SENTENCES, MAX_SENTENCE_LEN))\n",
    "        \n",
    "        sent_input = Input(shape=(MAX_SENTENCE_LEN,))\n",
    "        word_emb = Embedding(MAX_FEATURES+1, \n",
    "                        EMBED_SIZE,\n",
    "                        weights=[self.initial_weights],\n",
    "                        mask_zero=self.mask_zero,\n",
    "                        #embeddings_regularizer=regularizers.l2(self.embeddings_regularization),\n",
    "                        trainable=True)(sent_input)\n",
    "    \n",
    "        if self.mask_zero:\n",
    "            word_emb = ZeroMaskedLayer()(word_emb)\n",
    "        word_emb = SpatialDropout1D(self.spatial_drop)(word_emb)\n",
    "        if self.rnn_type == 'gru':\n",
    "            l_rnn = AttentionWrapper(GRU(self.rnn_word_dim, return_sequences=True, consume_less=\"mem\")(word_emb))\n",
    "            l_rnn = GlobalAveragePooling1D()(l_rnn)\n",
    "        sentEncoder = Model(sent_input, l_rnn)\n",
    "            \n",
    "        \n",
    "        emb = TimeDistributed(sentEncoder)(inp)\n",
    "        print(emb.shape)\n",
    "        if self.bidirectional:\n",
    "            enc = Bidirectional(CuDNNGRU(int(self.rnn_sent_dim), return_sequences=True, return_state=True, stateful=True,\n",
    "                                         ))(emb)\n",
    "            x = enc[0]\n",
    "            state = enc[1]\n",
    "        else:\n",
    "            x, state = GRU(int(self.rnn_sent_dim), return_sequences=True, return_state=True,\n",
    "                            kernel_regularizer=regularizers.l2(self.gru_kernel_regularization),\n",
    "                            recurrent_regularizer=regularizers.l2(self.gru_recurrent_regularization),\n",
    "                            bias_regularizer=regularizers.l2(self.gru_bias_regularization)\n",
    "                               )(emb)\n",
    "            #x = SpatialDropout1D(0.5)(x)\n",
    "        \n",
    "        if self.pool_type == 'avg':\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'max':\n",
    "            x = GlobalMaxPool1D()(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'attn':\n",
    "            x = AttentionLayer(MAX_SENTENCES)(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'all':\n",
    "            x1 = GlobalAveragePooling1D()(x)\n",
    "            x2 = GlobalMaxPool1D()(x)\n",
    "            x3 = AttentionLayer(MAX_SENTENCES)(x)\n",
    "            x = concatenate([x2, x3, state])\n",
    "    \n",
    "        x = Dropout(self.dropout)(x)\n",
    "        x = Dense(self.dense_dim)(x)\n",
    "        x = PReLU()(x)\n",
    "        \n",
    "        #x = Dense(self.dense_dim)(x)\n",
    "        #x = PReLU()(x)\n",
    "\n",
    "        out = Dense(self.out_dim, activation=\"sigmoid\")(x)\n",
    "        if self.optimizer == 'adam':\n",
    "            opt = Adam(lr=0.001, decay=0.0, clipnorm=1.0)\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            opt = RMSprop(clipnorm=1.0)\n",
    "        model = Model(inputs=inp, outputs=out)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        if self.callbacks:\n",
    "            self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose,\n",
    "                       callbacks=self.callbacks,\n",
    "                       shuffle=True)\n",
    "        else:\n",
    "            self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose,\n",
    "                       shuffle=True)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        if self.model:\n",
    "            y_hat = self.model.predict(X, batch_size=1024)\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(epoch):\n",
    "    if epoch == 0:\n",
    "        return 0.0015\n",
    "    if epoch == 1:\n",
    "        return 0.0001\n",
    "    if epoch == 2:\n",
    "        return 0.001\n",
    "    if epoch == 3:\n",
    "        return 0.00001\n",
    "\n",
    "\n",
    "def shuffle_crossvalidator(model, cvlist, X, y, lr_decay):\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    scores = []\n",
    "    LRDecay = LearningRateScheduler(lr_decay)\n",
    "\n",
    "    for tr_index, val_index in cvlist2:\n",
    "        X_tr, y_tr = X[tr_index, :], y[tr_index, :]\n",
    "        X_val, y_val = X[val_index, :], y[val_index, :]\n",
    "        RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "        model.set_params(**{'callbacks':[RocAuc, LRDecay]})\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        score = roc_auc_score(y_val, y_pred)\n",
    "        scores.append(score)\n",
    "        print(\"ROC AUC for this fold is \", score)\n",
    "        y_trues.append(y_val)\n",
    "        y_preds.append(y_pred)\n",
    "        K.clear_session()\n",
    "        gc.collect()\n",
    "        #break\n",
    "    y_trues = np.concatenate(y_trues)\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "    score = roc_auc_score(y_trues, y_preds)\n",
    "    print(\"Overall score on 10 fold CV is {}\".format(score))\n",
    "    \n",
    "    return y_preds, y_trues, scores\n",
    "\n",
    "def outoffold_crossvalidator(model_params, cvlist, X, y, lr_decay):\n",
    "    y_preds = np.zeros(y.shape)\n",
    "    LRDecay = LearningRateScheduler(lr_decay)\n",
    "\n",
    "    for tr_index, val_index in cvlist2:\n",
    "        X_tr, y_tr = X[tr_index, :], y[tr_index, :]\n",
    "        X_val, y_val = X[val_index, :], y[val_index, :]\n",
    "        RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "        \n",
    "        model.set_params(**{'callbacks':[RocAuc, LRDecay]})\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        print(\"ROC AUC for this fold is \", roc_auc_score(y_val, y_pred))\n",
    "        y_preds[val_idx] = y_pred\n",
    "        K.clear_session()\n",
    "        break\n",
    "    score = roc_auc_score(y, y_preds)\n",
    "    print(\"Overall score on 10 fold CV is {}\".format(score))\n",
    "    \n",
    "    return y_preds, y_trues, score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: UserWarning: Update your `GRU` call to the Keras 2 API: `GRU(300, return_sequences=True, implementation=1)`\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d9da02b77f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                     batch_size=100, epochs=2, optimizer='adam', mask_zero=False, pool_type='all')\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0my_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle_crossvalidator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcvlist2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-d978fbbf6dcb>\u001b[0m in \u001b[0;36mshuffle_crossvalidator\u001b[0;34m(model, cvlist, X, y, lr_decay)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'callbacks'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRocAuc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLRDecay\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4e9d9e7c98b9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-4e9d9e7c98b9>\u001b[0m in \u001b[0;36m_build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mword_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpatialDropout1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gru'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0ml_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_word_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsume_less\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0ml_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalAveragePooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_rnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0msentEncoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kaggle_competitions/toxic_comments/toxic_algo/AttentionLayer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, layer, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \"\"\"\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRecurrent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'consume_less'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AttentionLSTMWrapper doesn't support RNN's with consume_less='cpu'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(\n",
    "        device_count = {'GPU': 0}\n",
    "    )\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "model = LSTMGRNN(rnn_word_dim=300, rnn_sent_dim=100, dense_dim=900, initial_weights=embedding_matrix, bidirectional=False,\n",
    "                    batch_size=100, epochs=2, optimizer='adam', mask_zero=False, pool_type='all')\n",
    "\n",
    "y_preds, y_trues, _ = shuffle_crossvalidator(model, cvlist2, X_train, y, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
