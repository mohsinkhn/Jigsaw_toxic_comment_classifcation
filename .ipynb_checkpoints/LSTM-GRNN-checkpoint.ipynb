{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM-GRNN\n",
    "\n",
    "Implementation based on paper here:\n",
    "http://aclweb.org/anthology/D15-1167\n",
    "\n",
    "Inspiration for code taken from here:\n",
    "https://github.com/richliao/textClassifier/blob/master/textClassifierHATT.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, gc, numpy as np, pandas as pd\n",
    "import tensorflow as tf\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, Permute, GRU, Conv1D, LSTM, Embedding, Dropout, Activation, CuDNNLSTM, CuDNNGRU, concatenate, Flatten\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, BatchNormalization, SpatialDropout1D, Dot\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "import keras.backend as K\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "from functools import reduce\n",
    "from keras.layers import Layer, PReLU, SpatialDropout1D\n",
    "from keras import initializers\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize, TweetTokenizer, MWETokenizer, ToktokTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "np.random.seed(786)\n",
    "\n",
    "from SentenceTokenizer import SentenceTokenizer\n",
    "from ZeroMaskedLayer import ZeroMaskedLayer\n",
    "from AttentionLayer import AttentionLayer\n",
    "from RocAucEvaluation import RocAucEvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "utility_path = '../utility/'\n",
    "comp = 'jigsaw-toxic-comment-classification-challenge/'\n",
    "EMBEDDING_FILE=f'{utility_path}crawl-300d-2M.vec'\n",
    "TRAIN_DATA_FILE=f'{path}train.csv'\n",
    "TEST_DATA_FILE=f'{path}test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(series):\n",
    "    return series.apply(lambda s: unicodedata.normalize('NFKC', str(s)))\n",
    "\n",
    "STOP_WORDS = set(stopwords.words( 'english' ))\n",
    "\n",
    "repl = {\n",
    "    \"&lt;3\": \" good \",\n",
    "    \":d\": \" good \",\n",
    "    \":dd\": \" good \",\n",
    "    \":p\": \" good \",\n",
    "    \"8)\": \" good \",\n",
    "    \":-)\": \" good \",\n",
    "    \":)\": \" good \",\n",
    "    \";)\": \" good \",\n",
    "    \"(-:\": \" good \",\n",
    "    \"(:\": \" good \",\n",
    "    \"yay!\": \" good \",\n",
    "    \"yay\": \" good \",\n",
    "    \"yaay\": \" good \",\n",
    "    \"yaaay\": \" good \",\n",
    "    \"yaaaay\": \" good \",\n",
    "    \"yaaaaay\": \" good \",\n",
    "    \":/\": \" bad \",\n",
    "    \":&gt;\": \" sad \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" bad \",\n",
    "    \":(\": \" bad \",\n",
    "    \":s\": \" bad \",\n",
    "    \":-s\": \" bad \",\n",
    "    \"&lt;3\": \" heart \",\n",
    "    \":d\": \" smile \",\n",
    "    \":p\": \" smile \",\n",
    "    \":dd\": \" smile \",\n",
    "    \"8)\": \" smile \",\n",
    "    \":-)\": \" smile \",\n",
    "    \":)\": \" smile \",\n",
    "    \";)\": \" smile \",\n",
    "    \"(-:\": \" smile \",\n",
    "    \"(:\": \" smile \",\n",
    "    \":/\": \" worry \",\n",
    "    \":&gt;\": \" angry \",\n",
    "    \":')\": \" sad \",\n",
    "    \":-(\": \" sad \",\n",
    "    \":(\": \" sad \",\n",
    "    \":s\": \" sad \",\n",
    "    \":-s\": \" sad \",\n",
    "    #\"m\": \"am\",\n",
    "    #\"r\": \"are\",\n",
    "    #\"u\": \"you\",\n",
    "    \"haha\": \"ha\",\n",
    "    \"hahaha\": \"ha\",\n",
    "}\n",
    "\n",
    "#https://stackoverflow.com/questions/15175142/how-can-i-do-multiple-substitutions-using-regex-in-python\n",
    "def one_xlat(match):\n",
    "        return repl[match.group(0)]\n",
    "    \n",
    "rx = re.compile('|'.join(map(re.escape, repl)))\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(series):\n",
    "    series = unicodeToAscii(series)\n",
    "    series = series.str.lower()\n",
    "    series = series.str.replace(rx, one_xlat)\n",
    "    series = series.str.replace(r\"(\\n){1,}\", \" \")\n",
    "    series = series.str.replace(r\"\\'\", \"\")\n",
    "    series = series.str.replace(r\"\\-\", \"\")\n",
    "    series = series.str.replace(r\"[^0-9a-zA-Z.,!?]+\", \" \")\n",
    "    series = series.str.replace(r\"[.]+\",\".\")\n",
    "    series = series.str.replace(r\"[!]+\",\"!\")\n",
    "    series = series.str.replace(r\"[?]+\",\".\")\n",
    "    return series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8) (153164, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohsin/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:605: Warning: The least populated class in y has only 2 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "print(train.shape, test.shape)\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "\n",
    "#Get validation folds\n",
    "train['target_str'] = reduce(lambda x,y: x+y, [train[col].astype(str) for col in list_classes])\n",
    "train['target_str'] = train['target_str'].replace('110101', '000000').replace('110110','000000')\n",
    "cvlist1 = list(StratifiedKFold(n_splits=10, random_state=786).split(train, train['target_str'].astype('category')))\n",
    "cvlist2 = list(StratifiedShuffleSplit(n_splits=5, test_size=0.05, random_state=786).split(train, train['target_str'].astype('category')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in train, test:\n",
    "    df[\"comment_text\"] = normalizeString(df[\"comment_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 200000\n",
    "MAX_SENTENCE_LEN = 50\n",
    "MAX_SENTENCES = 10\n",
    "\n",
    "def custome_tokenizer(text):\n",
    "    return [TweetTokenizer().tokenize(sent) for sent in  sent_tokenize(text)]\n",
    "\n",
    "tok = SentenceTokenizer(max_features=MAX_FEATURES, max_sentence_len=MAX_SENTENCE_LEN, max_sentences=MAX_SENTENCES, tokenizer=custome_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c = Counter()\n",
    "#s = Counter()\n",
    "#def cnts(x):\n",
    "#    toks = custome_tokenizer(x)\n",
    "#    s.update([len(toks)])\n",
    "#    c.update([len(sent) for sent in toks])\n",
    "#train.comment_text.apply(lambda x: cnts(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 11s, sys: 523 ms, total: 2min 12s\n",
      "Wall time: 2min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = tok.fit_transform(train.comment_text)\n",
    "X_test = tok.transform(test.comment_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 10, 50) (153164, 10, 50)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 300\n",
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def initialize_embeddings(filename, tokenizer):\n",
    "    embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(filename))\n",
    "\n",
    "    word_index = tokenizer.vocab_idx\n",
    "    nb_words = min(MAX_FEATURES, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= MAX_FEATURES: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 300)\n",
      "0.002633644116134224 0.22155702444719066\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = initialize_embeddings(EMBEDDING_FILE, tok)\n",
    "print(embedding_matrix.shape)\n",
    "print(np.mean(embedding_matrix), np.std(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            0.852941\n",
       "severe_toxic     0.176471\n",
       "obscene          0.735294\n",
       "threat           0.029412\n",
       "insult           0.441176\n",
       "identity_hate    0.029412\n",
       "dtype: float64"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "class LSTMGRNN(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, rnn_word_dim=150, rnn_sent_dim=150, dense_dim=256, rnn_type=\"gru\", batch_size=128, epochs=2, bidirectional=False, \n",
    "                 pool_type='all', initial_weights=None, optimizer='adam' ,verbose=1, out_dim=6, callbacks=None,\n",
    "                spatial_drop=0.0, dropout=0.0, mask_zero=True, \n",
    "                gru_kernel_regularization = 0.0001,\n",
    "                gru_recurrent_regularization = 0.0001,\n",
    "                gru_bias_regularization = 0.0001,\n",
    "                embeddings_regularization = 0.0,\n",
    "                ):\n",
    "        \n",
    "        self.rnn_word_dim = rnn_word_dim\n",
    "        self.rnn_sent_dim = rnn_sent_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.rnn_type = rnn_type\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs= epochs\n",
    "        self.bidirectional = bidirectional\n",
    "        self.pool_type = pool_type\n",
    "        self.initial_weights = initial_weights\n",
    "        self.verbose = verbose\n",
    "        self.callbacks = callbacks\n",
    "        self.optimizer = optimizer\n",
    "        self.out_dim = out_dim\n",
    "        self.spatial_drop = spatial_drop\n",
    "        self.dropout = dropout\n",
    "        self.mask_zero = mask_zero\n",
    "        self.gru_kernel_regularization = gru_kernel_regularization\n",
    "        self.gru_recurrent_regularization = gru_recurrent_regularization\n",
    "        self.gru_bias_regularization = gru_bias_regularization\n",
    "        self.embeddings_regularization = embeddings_regularization\n",
    "        \n",
    "    def _build_model(self):\n",
    "        inp = Input(shape=(MAX_SENTENCES, MAX_SENTENCE_LEN))\n",
    "        \n",
    "        sent_input = Input(shape=(MAX_SENETNCE_LEN,))\n",
    "        word_emb = Embedding(MAX_FEATURES, \n",
    "                        EMBED_SIZE,\n",
    "                        weights=[self.initial_weights],\n",
    "                        mask_zero=self.mask_zero,\n",
    "                        #embeddings_regularizer=regularizers.l2(self.embeddings_regularization),\n",
    "                        trainable=True)(sent_inp)\n",
    "    \n",
    "        if self.mask_zero:\n",
    "            word_emb = ZeroMaskedLayer()(word_emb)\n",
    "        word_emb = SpatialDropout1D(self.spatial_drop)(word_emb)\n",
    "        if self.rnn_type == 'gru':\n",
    "            l_rnn = GRU(self.rnn_dim)(word_emb)\n",
    "        sentEncoder = Model(sent_input, l_rnn)\n",
    "            \n",
    "        \n",
    "        if self.bidirectional:\n",
    "            enc = Bidirectional(CuDNNGRU(int(self.gru_dim), return_sequences=True, return_state=True, stateful=True,\n",
    "                                         ))(emb)\n",
    "            x = enc[0]\n",
    "            state = enc[1]\n",
    "        else:\n",
    "            x, state = CuDNNGRU(int(self.gru_dim), return_sequences=True, return_state=True,\n",
    "                            kernel_regularizer=regularizers.l2(self.gru_kernel_regularization),\n",
    "                            recurrent_regularizer=regularizers.l2(self.gru_recurrent_regularization),\n",
    "                            bias_regularizer=regularizers.l2(self.gru_bias_regularization)\n",
    "                               )(emb)\n",
    "            #x = SpatialDropout1D(0.5)(x)\n",
    "        \n",
    "        if self.pool_type == 'avg':\n",
    "            x = GlobalAveragePooling1D()(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'max':\n",
    "            x = GlobalMaxPool1D()(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'attn':\n",
    "            x = AttentionLayer(MAX_LEN)(x)\n",
    "            x = concatenate([x, state])\n",
    "            \n",
    "        elif self.pool_type == 'all':\n",
    "            x1 = GlobalAveragePooling1D()(x)\n",
    "            x2 = GlobalMaxPool1D()(x)\n",
    "            x3 = AttentionLayer(MAX_LEN)(x)\n",
    "            x = concatenate([x2, x3, state])\n",
    "    \n",
    "        x = Dropout(self.dropout)(x)\n",
    "        x = Dense(self.dense_dim)(x)\n",
    "        x = PReLU()(x)\n",
    "        \n",
    "        #x = Dense(self.dense_dim)(x)\n",
    "        #x = PReLU()(x)\n",
    "\n",
    "        out = Dense(self.out_dim, activation=\"sigmoid\")(x)\n",
    "        if self.optimizer == 'adam':\n",
    "            opt = Adam(lr=0.001, decay=0.0, clipnorm=1.0)\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            opt = RMSprop(clipnorm=1.0)\n",
    "        model = Model(inputs=inp, outputs=out)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.model = self._build_model()\n",
    "        \n",
    "        if self.callbacks:\n",
    "            self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose,\n",
    "                       callbacks=self.callbacks,\n",
    "                       shuffle=True)\n",
    "        else:\n",
    "            self.model.fit(X, y, batch_size=self.batch_size, epochs=self.epochs,\n",
    "                       verbose=self.verbose,\n",
    "                       shuffle=True)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        if self.model:\n",
    "            y_hat = self.model.predict(X, batch_size=1024)\n",
    "        else:\n",
    "            raise ValueError(\"Model not fit yet\")\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
